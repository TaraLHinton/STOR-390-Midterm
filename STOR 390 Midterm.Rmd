---
title: 'In Harm''s Way: A Critical Analysis of Machine Learning and Ethical Allocation
  of Transitional Shelter Assistance'
author: "Tara Hinton"
date: "2024-10-23"
output:
  pdf_document: default
  html_document: default
  
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In the wake of a presidential disaster declaration, the Federal Emergency Management Administration (FEMA) can aid impacted survivors by providing temporary housing assistance for those without shelter via the Transitional Shelter Assistance (TSA) program (FEMA, 2021). This expense reimbursement is essential to directly meeting the needs of communities with structurally vulnerable housing and circumvent the costs of hotels and short-term rentals. In the context of a warming climate, increasing disaster severity poses greater displacement risk to populations in harm's way. In the "twin crisis" of climate change and poor housing infrastructure, efficient and just allocation of FEMA funds is an ever-more pressing issue (Rao & Hornstein, 2023). Currently, TSA eligibility is determined via home damage assessments, a process that can be both costly and time-consuming. This paper examines the methodology, conclusions, and ethical concerns of the study "Machine learning-based FEMA Transitional Shelter Assistance (TSA) eligibility prediction models" by Mahdi Afkhamiaghda and Emad Elwakil, in which the authors develop a supervised machine learning model for TSA eligibility prediction (2021). Their project, which aims to expedite TSA eligibility classification, raises numerous ethical concerns about equitable allocation of disaster aid and the automation of disaster decision-making. 

The authors begin their paper with an introduction to their study approach, the formulation of three models (logistic regression, decision tree, and KNN) to predict whether or not an applicant was TSA eligibile. The authors describe their data source, the National Emergency Management Information System, which shows 4.8 million registries of Individual Aid applications for survivors between 1998 and 2017. FEMA datasets often contain many extraneous variables, so dataset pre-processing occurred in several stages; first, authors reviewed and deemed several variables irrelevant, including city name, zip code, and census block ID. Second, numbers of missing values were calculated, and authors deleted rows in which no information was available for any variables, while numerical variables dependent on *empty boolean* variables were replaced with zeros rather than blanks. Next, researchers narrowed the scope of their project, focusing on North Carolina, Florida, and Texas, the three top states for Individual Assistance density. This study's final dataset, cleaned of outliers and normalized using a min-max scaling technique, considers eight predictors (*Roof damage amount*, *Household composition*, *Destroyed* (binary), *Residence type*, *Repair amount*, *Flood insurance* (binary), *Special needs* (binary), and *Foundation damage amount*). Min-max scaling is critical for models like KNN, SVM, and logistic regression, because it scales all features between 0 and 1 to ensure equal contribution to the models. 
 
The study addresses the formulation of its KNN model and appropriate selection of the K value. To best generalize the results and avoid overfitting, researchers performed a cross-validation to identify the best tuning parameter for K (K = 20). The authors visualized and inspected the probability mass functions and Kernel Density Estimation plot of the training data, yielding the result that around 60% of outcomes resulted in a denial of transitional housing assistance. The researchers also visualized all numerical variables  in a 3x3 matrix showing Kernel Density Estimation, which helped them understand the distributions of their training data. For example, *roofDamageAmount* was skewed to the left. Finally, the authors utilized a sample size of 100, which balanced computational complexity and predictive accuracy for all three models. To improve the accuracy of all three models and reduce overfitting, researchers employed a Chi-squared test for feature detection, then eliminating less important variables and rerunning the models. 

An accuracy, sensitivity, and precision confusion matrix indicated that supervised machine learning techniques like KNN and decision trees yielded better results than logistic regression, with KNN having the most consistent accuracy overall(70%). Accuracy indicates how often a classifier correctly predicts the outcome. Sensitivity shows the rate of correct predictions when the actual value is positive, while precision indicates how often prediction is correct when a positive value is predicted.  All three models have low sensitivity, meaning that they often under-predict the number of individuals who have received transitional shelter aid. Despite this, KNN models were found to have significantly higher sensitivity (16.80%) than logistic regression (2.98%) and decision trees (1.30%). The author's logistic regression model demonstrates a precision score (70.73%) than both the KNN and decision models (43% and 45% precision, respectively), so this model may consistently under-predict when participants receive TSA. In conclusion, the authors suggest a more robust effort to fill missing values within FEMA's dataset and point to other machine learning techniques, such as random forest, to further improve FEMA's TSA's eligibility prediction capabilities. 

By developing and assessing KNN and decision tree models to predict TSA eligibility, this study introduces critical ethical questions about the allocation of aid during climate crisis. In 2006, the Government Accountability Office released a report stating that an estimated 600 million to 1.4 billion dollars had been spend on improper individual
assistance (IA) payments(GAO, 2007). Currently, FEMA relies upon expert knowledge and home inspection to determine eligibility for TSA funds, a process often rife with racial and socioeconomic bias (Afkhamiaghda & Elwakil, 2021; FEMA, 2021). In addition to redlining, studies suggest that FEMA home appraisals have historically undervalued homes in Black communities, impacting the quality and amount of aid that homeowners may receive (FEMA, 2021). Further, pre-existing home damage in low-income settings may decrease the extent of damage that inspectors award post-disaster (FEMA, 2021). 

These historical disparities provoke compelling questions about the equitability of machine learning models given historical FEMA data. While a TSA eligibility model might help decision-makers expedite aid disbursement post-disaster, these models may be rooted in data that disadvantaged minority and low-income communities, raising concerns about inequality exacerbation. Other machine learning models, such as COMPAS, have been heavily critiqued for reproducing societal inequalities in decision outcomes. These models may feed in data like recidivism rates that are impacted by systemic inequalities. Similarly, FEMA data historically exhibits preference for white, wealthy homeowners -- so it is imperative that we question the statistical and ethical performance of this study. 

Over time, due to population growth and climate change, the need for post-disaster temporary shelter is projected to grow (Afkhamiaghda & Elwakil, 2021). The low sensitivities of Afkhamiaghda & Elwakil's models combined with climate change may point to the insufficiency in utilizing solely FEMA data to allocate aid for future needs in an increasingly disaster-prone world. As we elevate machine learning to solve bias and inefficiencies in aid allocation, the broader automation of disaster decision-making challenges our best and worst moral instincts. Should a need-based system prioritize allocations of aid based upon biased historical data? What does equitable look like in a disaster zone? On one hand,implementing machine learning in TSA allocation might free homeowners from the differential burden of poorly-trained and stressed damage inspectors. Conversely, on-site home visits allow damage inspectors to get a more complete picture of homeowner losses. While the authors of this paper tend toward a utilitarian framework of maximizing economic efficiency, applications of machine learning in moments of disaster must consider principles of distributive justice. 




References

Afkhamiaghda, Mahdi & Elwakil, Emad. "Machine learning-based FEMA Transitional Shelter Assistance (TSA) eligibility prediction models," *Journal of Emergency Management*, (2021): 10.5055/jem.2020.0000. 

FEMA. "FEMA: Transitional sheltering assistance." Available at
https://www.fema.gov/transitional-shelter-assistance. Accessed October
25, 2024

Kutz GD, Ryan Hurricanes Katrina and Rita disaster relief: Prevention is the key to minimizing fraud, waste, and abuse in recovery efforts, statement of Gregory Kutz, Managing Director Forensic Audits and Special Investigations, Testimony before the Committee on Homeland Security and Governmental Affairs, US Senate. United States Government Accountability Office, no. GAO-07-418T. United States Government Accountability Office, 2007.

Rao, Arya, & Shira Hornstein. “How The Twin Crises of Climate Change and Poor Public Housing Are Harming People’s Health.” STAT, August 29, 2023. https://www.statnews.com/2023/08/29/climate-change-public-housing-health-consequences/#:~:text=Millions%20of%20people%20are%20being,individual%20health%20of%20homeless%20individuals.

